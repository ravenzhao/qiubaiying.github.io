---
layout:     post  			# 使用的布局（不需要改）
title:      第十一章 特征选择和稀疏学习	# 标题 
subtitle:   机器学习    			# 副标题
date:       2018-10-04			# 时间
author:     RavenZhao	 		# 作者
header-img: img/post-bg-2015.jpg 	# 这篇文章标题背景图片
catalog: true 				# 是否归档
tags:					# 标签
    - 机器学习
    - 笔记


---

# 第十一章 特征学习和稀疏学习

### 11.1 子集搜索与评价

- 对一个学习任务来说，给定属性集，其中有些属性可能很关键、很有用，另一些属性则可能没什么用。
	- 对当前学习任务有用的属性称为“相关特征”（relevant feature）
	- 没什么用的属性称为“无关特征”（irrelevant feature）
	- 从给定的特征集合中选择出相关特征子集的过程，称为“特征选择”（feature selection）
- 特征选择是一个重要的“数据预处理”（data preprocessing）过程。
	- 在现实机器学习任务重，获得数据之后通常先进行特征选择，此后再训练学习器。
	- 现实任务中经常会遇到维数灾难问题：属性过多。
	- 去除不相关特征往往会降低学习任务的难度。
	- 特征选择的过程必须保证不丢失重要特征。
	- “冗余特征”（redundant feature）所包含的信息能从其他特征中推演出来。
- “子集搜索”（subset search）：
	- 前向（forward）搜索：给定特征集合$\{a_1,a_2,...,a_d\}$，可将每个特征看做一个候选子集，对这$d$个候选单特征子集进行评价，假定$\{a_2\}$最优，于是将$\{a_2\}$作为第一轮的选定集，假定在这$d-1$个候选两特征子集中$\{a_2,a_4\}$最优，且优于$\{a_2\}$，于是将$\{a_2,a_4\}$作为本轮的选定集；……假定在第$k+1$轮时，最优的候选$(k+1)$特征子集不如上一轮的选定集，则停止生成候选子集，并将上一轮选定的$k$特征集合作为特征选择结果。
	- 后向（backward）搜索：从完整的特征集合开始，每次尝试去掉一个无关特征。
	- 双向（bidirectional）搜索：将前向与后向搜索结合起来，每一轮逐渐增加选定相关特征（这些特征在后续轮中将确定不会被去除）、同时减少无关特征。
- 子集评价（subset evaluation）：给定数据集$D$，假定$D$中第$i$类样本所占的比例为$p_i(i=1,2,...,\lvert \mathcal{Y} \rvert)$。假定样本属性均为离散型，对属性子集$A$，假定根据其取值将$D$分成了$V$个子集$\{D^1,D^2,...,D^V\}$，每个子集中的样本在$A$上取值相同，计算属性子集$A$的信息增益：$Gain(A)=Ent(D)-\sum_{v=1}^V\frac{\lvert D^v\rvert}{\lvert D\rvert}Ent(D^v)$。
	- 信息熵定义为：$Ent(D)=-\sum_{i=1}^{\lvert\mathcal{Y}\rvert}p_klog_2p_k$
	- 信息增益$Gain(A)$越大，意味着特征子集$A$包含的有助于分类的信息越多。
	- 对每个候选特征子集，基于训练数据集$D$来计算其信息增益，作为评价准则。
- 更一般的，特征子集A实际上确定了对数据集D的一个划分，每个划分区域对应着A上的一个取值，而样本标记信息Y则对应着对D的真实划分，通过估算这两个划分的差异，就能对A进行评价。
	- 与Y对应的划分差异越小，则说明A越好。
- 特征选择方法大致可分为三类：
	- 过滤式（filter）
	- 包裹式（wrapper）
	- 嵌入式（embedding）