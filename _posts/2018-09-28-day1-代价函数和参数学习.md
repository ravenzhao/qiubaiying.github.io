---
layout:     post			# 使用的布局（不需要改）
title:      Day1-代价函数和参数学习	# 标题 
subtitle:   Coursera机器学习课程        	# 副标题
date:       2018-09-28			# 时间
author:     RavenZhao 			# 作者
header-img: img/post-bg-2015.jpg 	# 这篇文章标题背景图片
catalog: true 				# 是否归档
tags:					# 标签
- 机器学习
- 笔记
- Coursera

---

# Day 1.代价函数和参数学习算法

### 代价函数

- 定义线性回归假设函数为：$h_{(\theta)}(x)=\theta_0+\theta_1x$

- 代价函数为：$J(\theta_0,\theta_1)=\frac1{2m}\sum_{i=1}^m(h_{(\theta)}(x_i)-y_i)^2$

  - 此代价函数也被称为平方误差函数，或者平方误差代价函数。

- 目标函数为：$\mathop{min}\limits_{\theta_0,\theta_1}J(\theta_0,\theta_1)$

- 简化假设，若令$\theta_0=0$，则：

  - 代价函数为：$J(\theta_1)$
  - 目标函数为：$\mathop{min}\limits_{\theta_1}J(\theta_1)$
  - 假设函数与代价函数的比较：

  | $h_{\theta}(x)$                             | $J(\theta_1)$        |
  | ------------------------------------------- | -------------------- |
  | 确定参数$\theta_1$，假设函数为关于$x$的函数 | 关于$\theta_1$的函数 |
  | 线性函数                                    | 二次函数             |

### 参数学习

- 梯度下降法最小化代价函数$J(\theta_0,\theta_1)$
  - 初始化$\theta_0$与$\theta_1$的数值；
  - 不断改变$\theta_0$和$\theta_1$使得$J(\theta_0,\theta_1)$不断减小，直到达到最小值。

- 重复$\theta_j:=\theta_j-\alpha \frac{\part}{\part\theta_j}J(\theta_0,\theta_1)$，直至收敛。
  - $\alpha$为学习速率，在梯度下降算法中，控制步长。
  - $\theta_0$与$\theta_1$应当同时更新。
- $\frac{\part}{\part\theta_j}J(\theta_0,\theta_1)=\frac{\part}{\part\theta_j}\frac1{2m}\sum_{i=1}^m(h_{\theta}(x_i)-y_i)^2=\frac{\part}{\part\theta_j}\frac1{2m}\sum_{i=1}^m(\theta_0+\theta_1x_i-y_i)^2$
  - $\theta_0: \frac{\part}{\part\theta_0}J(\theta_0,\theta_1)=\frac1m\sum_{i=1}^m(h_{\theta}(x_i)-y_i)$
  - $\theta_1: \frac{\part}{\part\theta_1}J(\theta_0,\theta_1)=\frac1m\sum_{i=1}^m(h_{\theta}(x_i)-y_i)x_i$
- 重复以下运算，直至收敛：
  - $\theta_0:=\theta_0-\alpha\frac1m\sum_{i=1}^m(h_{\theta}(x_i)-y_i)$
  - $\theta_1:=\theta_1-\alpha\frac1m\sum_{i=1}^m(h_{\theta}(x_i)-y_i)x_i$
- 用于线性回归的代价函数是一个凸函数，因此其局部最小值即为全局最小值。
- 批量梯度下降：
  - 每一步梯度下降均使用全部训练样本。