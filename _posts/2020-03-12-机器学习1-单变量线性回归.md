---
layout:     post  			# 使用的布局（不需要改）
title:      机器学习		# 标题 
subtitle:   1-单变量线性回归    		# 副标题
date:       2020-03-12			# 时间
author:     RavenZhao	 		# 作者
header-img: img/post-bg-2015.jpg 	# 这篇文章标题背景图片
catalog: true 				# 是否归档
tags:					# 标签
    - 机器学习
    - 笔记

---

# 第1课 单变量线性回归

### 线性回归问题是什么？
线性回归问题，首先关注的名词是回归。回归则意味着模型最终输出的标签量（y值）是一个连续值，回归问题的本质可以说是基于一组特征数据来预测一个标签数据。

例如选用一组特征来描述一套房子（例如：户型、大小、位置、楼层、房龄……），根据特征值的不同，来预测这套房子的价格。
线性回归问题，第二个关注的名词是线性。线性也就是说我们接受这样一种假设，我们选择的特征变量，与所要预测的标签量之间存在线性关系。用函数的形式可以表达为：
$$h(x)=\theta_0+\theta_1x_1+\theta_2x_2+\ldots+\theta_nx_n$$
其中，${x_1,x_2,\ldots,x_n}$为描述样本的特征，$h(x)$代表模型的输出值，即对应训练样本中的标签$y$。

最简单的线性回归问题就是只有一个特征变量的问题，即单变量线性回归，其假设函数为：
$$h(x)=\theta_0+\theta_1x_1$$

在这个假设函数中，$\theta_i$表示参数，可以将参数理解为特征值相对预测结果$h(x)$的权重。而利用训练数据所要训练的也就是这些参数。

现在假设已经获取了一组训练数据，这组训练数据可以表示为$\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\ldots,(x^{(m)},y^{(m)})\}$，m表示样本的数量，$(x^{(i)},y^{(i)})$表示第$i$个样本。

在这样一个训练集上训练模型，最终会得到一组确定的$\theta_0,\theta_1$组合，但是训练的效果怎么样，还是需要通过一个指标来进行评价（也就是前面的性能测量P）。这里用来评价的方式是采用代价函数。

在讨论代价函数之前，还是要先从线性回归假设模型的训练目的说起，模型的训练目的是通过确定参数$\theta_0,\theta_1$的值，使得假设最后输出的结果$h(x)$能够与训练集最大程度的匹配。怎样才算是最大程度匹配，肯定是$h(x)$与训练集$y$之间的差距越小越好，放在整个训练集上，那就是模型的输出与训练集标签差的平方和要尽可能小，用函数表现为：
$$J(\theta_0,\theta_1)=\frac{1}{2m}\sum_{i=1}^{m}[h(x^{(i)})-y^{(i)}]^2$$
函数$J(\theta)$也被称为是代价函数，要想实现线性回归模型训练的目的，也就是需要让$J(\theta)$取到最小值，即：
$$\mathop{min}\limits_{\theta_0,\theta_1}J(\theta_0,\theta_1)=\mathop{min}\limits_{\theta_0,\theta_1}\frac{1}{2m}\sum_{i=1}^m(\theta_0+\theta_1x_1^{(i)}-y^{(i)})^2$$
这样的代价函数也被叫做平方差代价函数。

### 线性回归问题如何求解？
现在我们的目的就很明确了，利用$m$组训练样本数据，计算得到最合适的$\theta_0,\theta_1$组合。至于求解的方式，采用梯度下降算法（也叫批量梯度下降）。

梯度下降算法其实是一种优化方法，优化问题中：
目标函数为$\mathop{min}\limits_{\theta_0,\theta_1}J(\theta_0,\theta_1,\cdots,\theta_n)$

这里考虑的是一般情况，假设有$n$个特征值，再加$\theta_0$。
利用梯度下降算法求解参数的步骤，基本可以分为以下几步：

1. 