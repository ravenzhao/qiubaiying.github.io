---
layout:     post			# 使用的布局（不需要改）
title:      第三周-逻辑回归		# 标题 
subtitle:   Coursera机器学习课程        	# 副标题
date:       2018-10-03			# 时间
author:     RavenZhao 			# 作者
header-img: img/post-bg-2015.jpg 	# 这篇文章标题背景图片
catalog: true 				# 是否归档
tags:					# 标签
- 机器学习
- 笔记
- Coursera



---

# 第三周 逻辑回归

- 二元分类问题：$y=\begin{cases}0&\text{Negative Cases};\\1&\text{Positive Cases}\end{cases}$

  - 多元分类问题：$y\in\{0,1,2,3,...,m\}$

- 在分类问题中应用线性回归假设，往往不能取得好的效果。

- 逻辑回归是一个分类问题算法。$0\leq h_{\theta}(x)\leq 1$

	- 逻辑方程(Logistic Function; Sigmoid Funtion)$g(z)=\frac1{1+e^{-z}}$
	- 逻辑回归方程$h_{\theta}(x)=g(\theta^Tx)=\frac1{1+e^{-\theta^Tx}}$

	![image-20181003203849586](https://ws3.sinaimg.cn/large/006tNbRwly1fvvbffog1wj31bo08kta3.jpg)

	- $h_{\theta}(x)$得到的是根据输入值$x$计算$y=1$的概率。$h_{\theta}(x)=P(y=1\mid x;\theta)$
	- $P(y=1\mid x;\theta)+P(y=0\mid x;\theta)=1$

	- 决策边界，预测$\begin{cases}y=1&h_{\theta}(x)\geq0.5;\\y=0&h_{\theta}(x)<0.5.\end{cases}\Rarr\begin{cases}\theta^Tx\geq 0&y=1;\\\theta^Tx<0&y=0.\end{cases}$
	- 非线性决策边界：增加多项式高阶项

- 代价函数$J(\theta)$：$Cost(h_{\theta}(x),y)=\begin{cases}-log(h_{\theta}(x))&\text{if y=1;}\\-log(1-h_{\theta}(x))&\text{if y=0.}\end{cases}$

	- $Cost(h_{\theta}(x),y)=-ylog(h_{\theta}(x))-(1-y)log(1-h_{\theta}(x))$
	- $J(\theta)=\frac1m\sum_{i=1}^mCost(h_{\theta}(x),y)=-\frac1m\sum_{i=1}^m(ylog(h_{\theta}(x))+(1-y)log(1-h_{\theta}(x)))$

![image-20181003212553231](https://ws1.sinaimg.cn/large/006tNbRwly1fvvcsenibbj30hu0f2di9.jpg)

![image-20181003212617703](https://ws4.sinaimg.cn/large/006tNbRwly1fvvcstre5lj30hg0gcjth.jpg)

- 目标函数：$\mathop{min}\limits_{\theta}J(\theta)$
- 梯度下降，迭代$\theta_j:\theta_j-\alpha\sum_{i=1}^m(h_{\theta}(x)-y)x_j$

- 向量化计算：
	- $h=g(X\theta)$
	- $J(\theta)=\frac1m(-y^Tlog(h)-(1-y)^Tlog(1-h))$
	- $\theta:\theta-\frac{\alpha}{m}X^T(g(X\theta)-y)$
- 一些优化算法：（不需要手动选择学习速率；通常比梯度下降速度更快）
	- 共轭梯度法BFGS（变尺度法）
	- L-BFGS（限变尺度法）

### 多元分类问题

- 一对多（One-vs-All, One-vs-Rest）分类算法：
	- 将多元分类问题转化为多个二元分类问题。
	- $h^(i)_{\theta}(x)=P(y=i\mid x;\theta)$
	- 对于新的待预测对象，计算求解$\mathop{max}\limits_{i}h^{(i)}_{\theta}(x)$，最大值所对应的$i$即为其分类。

 ### 过拟合问题

- 过拟合（over-fitting）算法，方差（variance）高
	- 特征数量太多时，算法对训练集数据的拟合非常好，但是对于新数据的泛化性能很差。
- 欠拟合（under-fitting）算法，偏差（bias）高
- 解决过拟合问题的思路：
	- 减少特征数量。
		- 手动选择特征
		- 利用模型选择算法自动选择特征。
	- 正则化（regularization）技术改善过拟合问题。
		- 保留所有特征，降低或减少参数$\theta_j$的数量级。
- 正则化：
	- 降低参数$\theta$的数量级，能够简化假设，更容易避免过拟合。
	- 线性回归问题中，面临特征数量很大的问题时，可以对代价函数进行修改：$J(\theta)=\frac1{2m}[\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2+\lambda\sum_{i=1}^m\theta^2]$
		- $\lambda\sum_{i=1}^m\theta^2$为正则化项，用于缩小所有的$\theta$值。
		- $\lambda$为正则化参数，用于保持训练数据和保持参数量级较小之间的平衡。
- 梯度下降法解线性回归问题：
	- 正则化代价函数：$J(\theta)=\frac1{2m}[\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2+\lambda\sum_{i=1}^m\theta^2]$
	- 优化目标：$\mathop{min}\limits_{\theta}J(\theta)$
	- $\begin{cases}\theta_0:\theta_0-\frac{\alpha}{m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})x_0^{(i)}\\\theta_j:\theta_j-\alpha[\frac1{m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j]\end{cases}$（惩罚除$\theta_0$以外的参数值）
	- $\theta_j$可以改写为$\theta_j:\theta_j(1-\alpha\frac{\lambda}{m})-\frac{\alpha}{m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}$
		- 第一项将$\theta_j$略微缩小；
		- 第二项与之前的梯度下降算法完全相同。
- 正规方程法解线性回归问题：
	- $\Theta=(X^TX+\lambda\begin{bmatrix}0&0&\cdots&0\\0&1&\cdots&0\\0&0&\ddots&0\\0&0&\cdots&1\end{bmatrix})^{-1}X^Ty$
	- 如果样本数量≤特征数量$m\leq n$，则$X^TX$为不可逆矩阵（奇异矩阵singular），是一个退化（degenerate）的矩阵。
	- 在上述条件下，如果$\lambda>0$，则括号中的矩阵是可逆的。
- 梯度下降逻辑回归问题：  
	- 正则化后的代价函数：$J(\theta)=-\frac1m\sum_{i=1}^m(ylog(h_{\theta}(x))+(1-y)log(1-h_{\theta}(x)))+\frac{\lambda}{m}\sum_{j=1}^m\theta_j^2$
	- 参数更新与线性回归梯度下降类似：$\begin{cases}\theta_0:\theta_0-\frac{\alpha}{m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})x_0^{(i)}\\\theta_j:\theta_j-\alpha[\frac1{m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j]\end{cases}$，但是$h_{\theta}(x)$与线性回归不同。 