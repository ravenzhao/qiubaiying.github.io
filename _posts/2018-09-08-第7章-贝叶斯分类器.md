---
layout:     post   				    # 使用的布局（不需要改）
title:      第七章 贝叶斯分类器		# 标题 
subtitle:                            #副标题
date:       2018-09-08 				# 时间
author:     RavenZhao 				# 作者
header-img: img/post-bg-2015.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 机器学习
    - 笔记
---

# 第七章 贝叶斯分类器

### 7.1 贝叶斯决策论

- 贝叶斯决策论（Bayesian Decision Theory）是概率框架下实施决策的基本方法。
  - 在所有相关概率都已知的理想情形下
  - 考虑如何基于已知概率和误判损失来选择最优的类别标记。
- 假设有$N$中可能的类别标记，即$Y=\{c_1,c_2,...,c_N\}$，$\lambda_{ij}$是将一个真实标记为$c_j$的样本误分类为$c_i$所产生的损失，基于后验概率$P(c_i|x)$可获得将样本$x$分类为$c_i$所产生期望损失（expected loss），即在样本$x$上的条件风险（conditional risk）：$R(c_i|x)=\sum_{j=1}^N\lambda_{ij}P(c_j|x)$
  - 寻找一个判定准则：$h:X\rarr Y$以**最小化总体风险**$R(h)=E_x[R(h(x)|x)]$
  - 对每个样本$x$，若$h$能最小化条件风险$R(h(x)|x)$，则总体风险$R(h)$也将被最小化。
  - 贝叶斯判定准则（Bayes decision rule）：为最小化总体风险，只需在每个样本上选择那个**能使条件风险$R(c|x)$最小**的类别标记。即$h^*(x)=\mathop{arg\text{ min}}\limits_{c\in Y}R(c|x)$
  - $h^*$称为贝叶斯最优分类器（Bayes optimal classifier），与之对应的总体风险$R(h^*)$称为贝叶斯风险（Bayes risk）。$1-R(h^*)$反映了分类器所能达到的最好性能，即通过机器学习所能产生的模型精度的理论上限。
    - 若目标是最小化分类错误率，则误判损失$\lambda_{ij}$可写为$\lambda_{ij}=\begin{cases}0,&\text{if i=j;}\\1,&\text{otherwise}\end{cases}$
    - 此时风险条件$R(c|x)=1-P(c|x)$
    - 最小化分类错误率的贝叶斯最优分类器为$h^*(x)=\mathop{arg\text{ max}}\limits_{c\in Y}P(c|x)$，即对每个样本$x$，选择能使后验概率$P(c|x)$最大的类别标记。
- 使用贝叶斯判定准则来最小化决策风险，首先要获得后验概率$P(c|x)$。然而，现实任务中通常难以直接获得。机器学习要实现的是基于有限的训练样本集尽可能的准确估计后验概率$P(c|x)$。有两种策略：
  - 给定$x$，通过直接建模$P(c|x)$来预测$c$，这样得到的是“判别式模式”（discriminative models）：决策树、BP神经网络、支持向量机
  - 先对联合概率分布$P(x,c)$建模，然后由此获得$P(c|x)$，这样得到的是“生成式模型”（generative models）。
    - $P(c|x)=\frac{P(x,c)}{P(x)}$
    - 基于贝叶斯定理，$P(c|x)$可以写为$P(c|x)=\frac{P(c)P(x|c)}{P(x)}$，其中$P(c)$是类“先验”（prior）概率；$P(x|c)$是样本$x$相对于类标记$c$的类条件概率（class-conditional probability），或称为似然（likelihood）；$P(x)$是用于归一化的“证据”（evidence）因子。
    - 给定样本$x$，证据因子$P(x)$与类标记无关，因此估计$P(c|x)$的问题转化为如何给予训练数据$D$来估计先验$P(c)$和似然$P(c|x)$。
      - 类先验概率$P(c)$表达了样本空间中各类样本所占的比例，根据大数定律，当训练集包含充足的独立同分布样本时，$P(c)$可以通过各类样本出现的频率来进行估计
      - 对类条件概率$P(c|x)$来说，它涉及关于$x$所有属性的联合概率，直接根据样本出现的频率来估计会遇到严重困难。

### 7.2 极大似然估计

- 估计类条件概率的一种常用策略是先假定其具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计。
  - 记关于类别$c$的类条件概率为$P(x|c)$，假设$P(x|c)$具有确定的形式并且被参数向量$\theta_c$唯一确定，则我们的任务就是利用训练集$D$估计参数$\theta_c$。
  - $P(x|c)$可以被记为$P(x|\theta_c)$。
- 概率模型的训练过程就是参数估计（parameter estimation）过程，对于参数估计，统计学两个学派提供了不同的解决方案：
  - 频率主义学派（Frequentist）：认为参数虽然未知，但却是客观存在的固定值。
  - 贝叶斯学派（Bayesian）：认为参数是未观察到的随机变量，其本身也可有分布。可以假定参数服从一个先验分布，然后基于观测到的数据来计算参数的后验分布。
- 极大似然估计（Maximum Likelihood Estimation，简称MLE）源自于频率主义学派，根据数据采样来估计概率分布参数。
  - 令$D_c$表示训练集$D$中第$c$类样本组成的集合，假设这些样本是独立同分布的，则参数$\theta_c$对于数据集$D_c$的似然是$P(D_c|\theta_c)=\prod_{x\in {D_c}}P(x|\theta_c)$
  - 对$\theta_c$进行极大似然估计，就是寻找能最大化似然$P(D_c|\theta_c)$的参数值$\hat\theta_c$。直观上看，极大似然估计是试图在$\theta_c$所有可能的取值中，找到一个能使数据出现的“可能性”最大的值。
  - 连乘操作易造成下溢，通常使用对数似然（log-likelihood）$LL(\theta_c)=logP(D_c|\theta_c)=\sum_{x\in {D_c}}logP(x|\theta_c)$
    - 此时参数$\theta_c$的极大似然估计$\hat\theta_c$为$\hat\theta_c=\mathop{\text{arg max}}\limits_{\theta_c}LL(\theta_c)$
  - 这种参数化的方法能使类条件概率估计变得相对简单
  - 估计结果的准确性严重依赖所假设的概率分部形式是否符合潜在的真实数据分布。

### 7.3 朴素贝叶斯分类器

- 基于贝叶斯公式来估计后验概率$P(c|x)$的主要困难再与：类条件概率$P(x|c)$ 是所有属性上的联合概率。难以从有限的训练样本直接估计得到。
- 朴素贝叶斯分类器（naive Bayes classifier）采用了“属性条件独立性假设”（attribute conditional independence assumption）：对已知类别，假设所有属性相互独立。
- 基于属性条件独立性假设，可以得到$P(c|x)=\frac{P(c)P(x|c)}{P(x)}=\frac{P(c)}{P(x)}\prod_{i=1}^dP(x_i|c)$
  - 其中$d$为属性数目
  - $x_i$为$x$在第$i$个属性上的取值。
- 基于上式得到贝叶斯判定准则$h_{nb}(x)=\mathop{\text{arg max}}\limits_{c\in Y}P(c)\prod_{i=1}^dP(x_i|c)$，此为朴素贝叶斯分类器的表达式。
- 朴素贝叶斯分类器的训练过程就是基于训练集$D$来估计类先验概率$P(c)$，并为每个属性估计条件概率$P(x_i|c)$。
  - 令$D_c$表示训练集$D$中第$c$类样本组成的集合，若有充足的独立同分布样本，则可容易地估计出类先验概率$P(c)=\frac{|D_c|}{|D|}$
  - 对离散属性而言，令$D_{c,x_i}$表示$D_c$中第$i$个属性上取值为$x_i$的样本组成的集合，则条件概率$P(x_i|c)$可估计为$P(x_i|c)=\frac{|D_{c,x_i}|}{D_c}$
  - 对连续属性可考虑概率密度函数，假定$p(x_i|c)~N(\mu_{c,i},\sigma _{c,i}^2)$，其中$\mu_{c,i}$和$\sigma^2_{c,i}$分别是第$c$类样本在第$i$个属性上取值的均值和方差，则有：$p(x_i|c)=\frac1{\sqrt{2\pi}\sigma_{c,i}}exp(-\frac{(x_i-\mu_{c,i})^2}{2\sigma_{c,i}^2})$
  - 为避免其他属性携带的信息被训练集中未出现的属性“抹去”，在估计概率值时通常要进行“平滑”，常用“拉普拉斯修正”（Laplacian correction）

### 7.4 半朴素贝叶斯分类器

- 为了降低贝叶斯公式中估计后验概率$P(c|x)$的困难，朴素贝叶斯分类器采用了属性条件独立性假设，但在现实任务中这个假设往往很难成立。

- 对属性条件独立性假设进行一定程度的放松，产生“半朴素贝叶斯分类器”（semi-naive Bayes classifier）

  - 基本想法：适当考虑一部分属性间的相互依赖信息，从而既不需完全进行联合概率计算，又不至于彻底忽略了比较强的属性依赖关系。
  - “独依赖估计”（One-Dependent Estimator 简称ODE）是半朴素贝叶斯分类器最常见的一种策略。“独依赖”就是假设每个属性在类别之外最多仅依赖于一个其他属性，即$P(c|x)\propto P(c)\prod_{i=1}^d P(x_i|c,pa_i)$
    - 其中$pa_i$为属性$x_i$所依赖的属性，称为$x_i$的父属性。
    - 对每个属性$x_i$，若其父属性$pa_i$已知，则可以估计概率值$P(x_i|c.pa_i)$
    - 问题的关键在于如何确定每个属性的父属性。

- 最直接的做法是假设所有属性都依赖于同一个属性，称为“超父”（super-parent），然后通过交叉验证等模型选择方法来确定超父属性，SPODE（Super-Parent ODE）方法。

  ![1536726752639](assets/1536726752639.png)

- TAN（Tree Augmented naive Bayes）是在最大带权生成树算法的基础上，将属性间依赖关系约简为树形结构。

  ![1536726770451](assets/1536726770451.png)

- AODE（Averaged One-Dependent Estimator）是一种基于机器学习机制、更为强大的独依赖分类器。AODE尝试将每个属性作为超父来构建SPODE，然后将那些具有足够训练数据支撑的SPODE集合起来作为最终结果。
