---
layout:     post   			# 使用的布局（不需要改）
title:      第八章 集成学习		# 标题 
subtitle:                            	#副标题
date:       2018-09-14 			# 时间
author:     RavenZhao 			# 作者
header-img: img/post-bg-2015.jpg 	#这篇文章标题背景图片
catalog: true 				# 是否归档
tags:					#标签
    - 机器学习
    - 笔记

---

# 第八章 集成学习

### 8.1 个体与集成

- 集成学习（ensemble learning）通过构建并结合多个学习器来完成学习任务，有时也被称为多分类器系统（multi-classifier system）、基于委员会的学习（committee-based learning）等。

  ![image-20180914202304629](https://ws2.sinaimg.cn/large/0069RVTdgy1fv9ce38yi9j30ie09ewfl.jpg)

  - 先产生一组“个体学习器”（individual learner），再用某种策略将它们结合起来。
  - 个体学习器通常由一个现有的学习算法从训练数据产生，例如C4.5决策树算法、BP神经网络算法等。
  - 同质（homogeneous）集成：集成中只包含同种类型的个体学习器（亦称“基学习器”base learner），例如“决策树集成”中全是决策树，“神经网络集成”中全是神经网络。相应的学习算法称为“基学习算法”（base learning algorithm）
  - 异质（heterogeneous）集成：集成中包含不同类型的个体学习器（称为“组件学习器”component learner）。

- 集成学习通过将多个学习器进行结合，长可获得比单一学习器显著优越的泛化性能。

- 要获得好的集成效果，个体学习器应“好而不同”，即个体学习器要有一定的准确性，即学习器不能太坏，并且要有多样性（diversity），即学习器之间具有差异。

- 在现实中，个体学习器的“准确性”和“多样性”本身存在冲突。

  - 准确性很高之后，要增加多样性就要牺牲准确性
  - 如何产生“好而不同”的个体学习器，是集成学习研究的核心。

- 集成学习方法大致分为两大类：

  - 序列化方法：个体学习器间存在强依赖关系、必须串行生成。代表为Boosting
  - 并行化方法：个体学习器间不存在强依赖关系，可同时生成。代表为Bagging和“随机森林”（Random Forest）。

### 8.2 Boosting

- Boosting是一族可将弱学习器提升为强学习器的算法。

  - 先从初始训练集训练出一个基学习器；
  - 根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注；
  - 基于调整后的训练样本来训练下一个基学习器；
  - 如此重复，直至基学习器书目达到事先指定的值T，最终将T个基学习器进行加权结合。

- AdaBoost算法，基于“加性模型”，即基学习器的线性组合来最小化指数损失函数（exponential loss function）。

  - 基学习器的线性组合：$H(x)=\sum_{t=1}^T\alpha_th_t(x)$
  - 指数损失函数：$l_{exp}(H|D)=E_{x\sim D}[e^{-f(x)H(x)}]$

  ![image-20180916192710169](https://ws3.sinaimg.cn/large/006tNc79ly1fvbmjfc5enj30qc0kwdk9.jpg)
  - $sign(H(x))=sign(\frac12ln\frac{P(f(x)=1|x)}{P(f(x)=-1|x)})=\begin{cases}1,&P(f(x)=1|x)>P(f(x)=-1|x)\\-1,&P(f(x)=1|x)<P(f(x)=-1|x)\end{cases}=\mathop{argmax}\limits_{y\in\{-1,1\}}P(f(x)=y|x)$
  - 若指数损失函数最小化，则分类错误率也将最小化。指数损失函数是分类任务原本0/1损失函数的一致的（consistent）替代损失函数。

- Boosting主要关注降低偏差，能给予泛化性能相当弱的学习器构建出很强的集成。