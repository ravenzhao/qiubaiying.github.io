---
layout:     post			# 使用的布局（不需要改）
title:      Day4-分类问题		# 标题 
subtitle:   Coursera机器学习课程        	# 副标题
date:       2018-10-03			# 时间
author:     RavenZhao 			# 作者
header-img: img/post-bg-2015.jpg 	# 这篇文章标题背景图片
catalog: true 				# 是否归档
tags:					# 标签
- 机器学习
- 笔记
- Coursera



---

# Day 4.分类问题

- 二元分类问题：$y=\begin{cases}0&\text{Negative Cases};\\1&\text{Positive Cases}\end{cases}$
	- 多元分类问题：$y\in\{0,1,2,3,...,m\}$

- 在分类问题中应用线性回归假设，往往不能取得好的效果。

- 逻辑回归是一个分类问题算法。$0\leq h_{\theta}(x)\leq 1$

	- 逻辑方程(Logistic Function; Sigmoid Funtion)$g(z)=\frac1{1+e^{-z}}$
	- 逻辑回归方程$h_{\theta}(x)=g(\theta^Tx)=\frac1{1+e^{-\theta^Tx}}$

	![image-20181003203849586](https://ws3.sinaimg.cn/large/006tNbRwly1fvvbffog1wj31bo08kta3.jpg)

	- $h_{\theta}(x)$得到的是根据输入值$x$计算$y=1$的概率。$h_{\theta}(x)=P(y=1\mid x;\theta)$
	- $P(y=1\mid x;\theta)+P(y=0\mid x;\theta)=1$

	- 决策边界，预测$\begin{cases}y=1&h_{\theta}(x)\geq0.5;\\y=0&h_{\theta}(x)<0.5.\end{cases}\Rarr\begin{cases}\theta^Tx\geq 0&y=1;\\\theta^Tx<0&y=0.\end{cases}$
	- 非线性决策边界：增加多项式高阶项

- 代价函数$J(\theta)$：$Cost(h_{\theta}(x),y)=\begin{cases}-log(h_{\theta}(x))&\text{if y=1;}\\-log(1-h_{\theta}(x))&\text{if y=0.}\end{cases}$

	- $Cost(h_{\theta}(x),y)=-ylog(h_{\theta}(x))-(1-y)log(1-h_{\theta}(x))$
	- $J(\theta)=\frac1m\sum_{i=1}^mCost(h_{\theta}(x),y)=-\frac1m\sum_{i=1}^m(ylog(h_{\theta}(x))+(1-y)log(1-h_{\theta}(x)))$

![image-20181003212553231](https://ws1.sinaimg.cn/large/006tNbRwly1fvvcsenibbj30hu0f2di9.jpg)

![image-20181003212617703](https://ws4.sinaimg.cn/large/006tNbRwly1fvvcstre5lj30hg0gcjth.jpg)

- 目标函数：$\mathop{min}\limits_{\theta}J(\theta)$
- 梯度下降，迭代$\theta_j:\theta_j-\alpha\sum_{i=1}^m(h_{\theta}(x)-y)x_j$

- 向量化计算：
	- $h=g(X\theta)$
	- $J(\theta)=\frac1m(-y^Tlog(h)-(1-y)^Tlog(1-h))$
	- $\theta:\theta-\frac{\alpha}{m}X^T(g(X\theta)-y)$
- 一些优化算法：（不需要手动选择学习速率；通常比梯度下降速度更快）
	- 共轭梯度法BFGS（变尺度法）
	- L-BFGS（限变尺度法）

### 多元分类问题

- 一对多（One-vs-All, One-vs-Rest）分类算法：
	- 将多元分类问题转化为多个二元分类问题。
	- $h^(i)_{\theta}(x)=P(y=i\mid x;\theta)$
	- 对于新的待预测对象，计算求解$\mathop{max}\limits_{i}h^{(i)}_{\theta}(x)$，最大值所对应的$i$即为其分类。

 