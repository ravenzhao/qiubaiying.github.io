---
layout:     post   				# 使用的布局（不需要改）
title:      第十章 降维与度量学习			# 标题 
subtitle:                      			# 副标题
date:       2018-09-28				# 时间
author:     RavenZhao	 			# 作者
header-img: img/post-bg-2015.jpg 	# 这篇文章标题背景图片
catalog: true 						# 是否归档
tags:							# 标签
    - 机器学习
    - 笔记

---

# 第十章 降维与度量学习

### 10.1 k近邻学习

- $k$近邻（k-NearestNeighbor，简称KNN）学习是一种常见的监督学习方法。
  - 给定测试样本，基于某种距离度量找出训练集中与其最靠近的k个训练样本，然后基于这k个“邻居”的信息来进行预测。
  - 在分类任务中常使用“投票法”，即选择这k个样本中出现最多的类别标记作为预测结果；
  - 在回归任务中可使用“平均法”，即将这k个样本的实值输出标记的平均值作为预测结果。
- k近邻学习是“懒惰学习”（lazy learning）的代表：在训练阶段仅仅把样本保存起来，待收到测试样本后再进行处理。（在训练阶段就对样本进行学习处理的方法称为“急切学习”eager learning）

![image-20180928215354483](https://ws1.sinaimg.cn/large/006tNc79ly1fvpli1as9vj30vg0cmn1f.jpg)

- 最近邻分类器虽然简单，但它的繁华错误率不超过贝叶斯最优分类器的错误率的两倍。

### 10.2 低维嵌入

- k近邻学习基于一个重要假设：任意测试样本$x$附近任意小的$\delta$距离范围内总能找到一个训练样本，即训练样本的采样密度足够大，称为“密采样”（dense sample）
- 高维情形下出现的数据样本稀疏、距离计算困难等问题，是所有机器学习方法共同面临的严重障碍，被称为“维数灾难”（curse of dimensionality）
  - 缓解途径是降维（dimension reduction），亦称“维数约简”。通过某种数学变换将原始高维属性空间转变为一个低维“子空间”（subspace）
  - 在子空间中样本密度大幅提高，距离计算也变得更为容易。

![image-20180929203834572](https://ws4.sinaimg.cn/large/006tNc79ly1fvqoxxwq1ej30pi0d6n4l.jpg)

- 多维缩放（Multiple Dimensional Scaling，简称DMS）是一种经典降维方法。
  - 假定$m$个样本在原始空间的距离矩阵为$D\in \mathbb{R}^{m\times m}$，其第$i$行第$j$列的元素$dist_{ij}$为样本$x_i$到$x_j$的距离。目标是获得样本在$d’$维空间的表示$Z\in \mathbb{R}^{d’\times m},d’\leq d$，且任意两个样本在$d’$维空间中的欧式距离等于原始空间中的距离，即$\lVert z_i-z_j\rVert=dist_{ij}$
  - 令$B=Z^TZ\in\mathbb{R}^{m\times m}$，其中$B$为降维后样本的内积矩阵，$b_{ij}=z_i^Tz_j$
    - $dist^2_{ij}=\lVert z_i\rVert^2+\lVert z_j\rVert^2-2z_i^Tz_j=b_{ii}+b_{jj}-2b_{ij}$
    - 令降维后的样本$Z$被中心化，即$\sum_{i=1}^mz_i=0$。显然，矩阵$B$的行与列之和均为零，即$\sum_{i=1}^mb_{ij}=\sum_{j=1}^mb_{ij}=0$ 。
    - $\sum_{i=1}^mdist_{ij}^2=tr(B)+mb_{jj}$
    - $\sum_{j=1}^mdist_{ij}^2=tr(B)+mb_{ii}$
    - $\sum_{i=1}^m\sum_{j=1}^mdist_{ij}^2=2mtr(B)$
    - $tr(·)$表示矩阵的迹（trace），$tr(B)=\sum_{i=1}^m\lVert z_i\rVert^2$，令
      - $dist^2_{i·}=\frac1m\sum_{j=1}^mdist^2_{ij}$
      - $dist^2_{·j}=\frac1m\sum_{i=1}^mdist^2_{ij}$
      - $dist^2_{··}=\frac1{m^2}\sum_{i=1}^m\sum_{j=1}^mdist_{ij}^2$
  - 得到$b_{ij}=-\frac12(dist^2_{ij}-dist^2_{i·}-dist^2_{·j}+dist^2_{··})$，由此即可通过降维前后保持不变的距离矩阵D求取内积矩阵B。
    - 对矩阵B做特征值分解（eigenvalue decomposition），$B=V\Lambda V^T$，其中$\Lambda=diag(\lambda_1,\lambda_2,...,\lambda_d)$为特征值构成的对角矩阵，$\lambda_1\geq\lambda_2\geq…\geq\lambda_d$，$V$为特征向量矩阵。
    - 假定其中有$d^{\ast}$个非零特征值，它们构成对角矩阵$\Lambda_{\ast}=diag(\lambda_1,\lambda_2,...,\lambda_d)$，令$V_{\ast}$表示相应的特征向量矩阵，则$Z$可表达为：$Z=\Lambda_{\ast}^{1/2}V_{\ast}^T\in\mathbb{R}^{d^\ast\times m}$
  - 在现实中为了有效降维，仅需降维后的距离与原始空间中的距离尽可能接近，不必严格相等。可取$d’\ll d$个最大特征值构成对角矩阵$\tilde \Lambda=diag(\lambda_1,\lambda_2,...,\lambda_d')$，令$\tilde V$表示相应的特征向量矩阵，则$Z=\tilde\Lambda^{1/2}\tilde V^T\in\mathbb{R}^{d’\times m}$

![image-20180929214203728](https://ws1.sinaimg.cn/large/006tNc79ly1fvqqrzr2xsj30te0bkjv9.jpg)

- 欲获得低维子空间，最简单的是对原始高维空间进行线性变换。给定$d$维空间中的样本$X=(x_1,x_2,...,x_m)\in\mathbb{R}^{d\times m}$，变换之后得到$d’\ll d$维空间中的样本$Z=W^TX$，其中$W\in\mathbb{R}^{d\times d’}$是变换矩阵，$Z\in\mathbb{R}^{d’\times m}$是样本在新空间中的表达。
  - 变换矩阵$W$可视为$d’$个$d$维基向量，$z_i=W^Tx_i$是第$i$个样本与这$d’$个基向量分别做内积而得到的$d’$维属性向量。
  - $z_i$是原属性向量$x_i$在新坐标系$\{w_1,w_2,...,w_{d'}\}$中的坐标向量。若$w_i$与$w_j（i\neq j）$正交，则新坐标系是一个正交坐标系，此时$W$为正交变换。新空间中的属性是原空间中属性的线性组合。
- 基于线性变换来进行降维的方法称为线性降维方法。