---
layout:     post   				    # 使用的布局（不需要改）
title:      第二章 模型评估与选择		# 标题 
subtitle:                            #副标题
date:       2018-08-18 				# 时间
author:     RavenZhao 				# 作者
header-img: img/post-bg-2015.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 机器学习
    - 笔记
---

# 第二章 模型评估与选择

### 2.1 经验误差与过拟合

- 错误率（error rate）:分类错误的样本数占样本总数的比例。如果在$m$个样本中有$a$个样本分类错误，则错误率$ E = a/m $。
- 精度（accuracy）= 1-错误率。$ accuracy = 1 - E $。
- 误差（error）：学习器的实际预测输出与样本的真实输出之间的差异。
  - 学习器在训练集上的误差称为训练误差（training error）或经验误差（empirical error）：我们希望得到泛化误差小的学习器。然而我们不知道新样本是什么样，实际能做的就是努力使经验误差最小化。
  - 学习器在新样本上的误差称为泛化误差（generalization error）。
- 过拟合（overfitting）：学习器把训练样本自身的一些特点当做了所有潜在样本都具有的一般特质，导致泛化性能下降。也叫过配。
  - 过拟合是机器学习所面临的关键障碍。
  - 过拟合是无法彻底避免的，只能缓解。

### 2.2 评估方法

- 通过实验测试对学习器的泛化误差进行评估。
  - 使用测试集（testing set）来测试学习器对新样本的判别能力
  - 以测试集上的测试误差（testing error）作为泛化误差的近似。
  - 测试集应尽可能与训练集互斥。

#### 2.2.1 留出法（hold-out）

直接将数据集$D$划分为两个互斥的集合，一个作为训练集$S$，一个作为测试集$T$。即$ D = S \bigcup T$，$ S \bigcap T = \emptyset $。

- 训练/测试集的划分要尽可能保持**数据分布的一致性**。
- 在分类任务中至少要保持**样本的类别比例相似**。（分层采样stratified sampling）
- 在使用留出法时，一般采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。
- 一般大约将2/3~4/5的样本用于训练，剩余样本用于测试。

#### 2.2.2 交叉验证法（cross validation）

- 也叫k折交叉验证（k-fold cross validation）
- 先将数据集$D$划分为$k$个大小相似的互斥子集，即$ D = D_1 \bigcup D_2 \bigcup ... \bigcup D_k, D_i \bigcap D_j = \emptyset (i ≠j)$。每个子集$ D_i $都尽可能保持分布的一致。
- 每次用$k-1$个子集的并集作为训练集，余下的那个子集作为测试集；获得$k$组训练/测试集，进行$k$次训练和测试。
- 返回$k$个测试结果的均值。
- $k$常用取值为10，此时称为10折交叉验证。其他常用$k$值为5，20等。
- 为减小因样本划分不同而引入的差别，$k$折交叉验证通常要随机使用不同的划分重复$p$次，最终评估结果是$p$次$k$折交叉验证的均值。
- 若数据集$D$包含$m$个样本，令$k=m$，得到交叉验证法的一个特例，留一法（leave-one-out，LOO）。
  - 留一法不受随机样本划分方式的影响。每个子集包含一个样本。
  - 数据集比较大的时候，训练$m$个模型的计算开销非常大。

#### 2.2.3 自助法

- 以自助采样法（bootstrap sampling）为基础，给定包含$m$个样本的数据集$D$，对它进行采样产生数据集$D'$：
  - 每次随机从$D$中挑选一个样本，将其拷贝放入$D'$
  - 然后再将该样本放回初始数据集$D$中，使得该样本在下次采样时仍有可能被采集到。
  - 这个过程重复$m$次，就得到包含$m$个样本的数据集$D'$。
- 通过自助采样，初始数据集$D$中约有36.8%的样本未出现在采样数据集$D'$中。
- 将$D'$作为训练集，将$D-D'$用作测试集。
- 实际评估的模型与期望评估的模型都用$m$个训练样本。
- 数据总量的1/3的，没在训练集中出现的样本用于测试。这样的测试结果，称为“外包估计”（out-of-bag estimate）。
- 数据集较小，难以有效划分训练/测试集时很有用。
- 改变了初始数据集的分部，会引入估计偏差，在初始数据量充足时，留出法和交叉验证法更常用。

#### 2.2.4 调参和最终模型

- 调参（parameter tuning）：
  - 对每个参数选定一个范围和变化步长。
  - 计算开销和性能估计之间折中选择。
- 测试数据：学得模型在实际使用中遇到的数据。
- 验证集（validation set）：魔性评估与选择中用于评估测试的数据集。

### 2.3 性能度量（Performance Measure）

- 衡量模型泛化能力的评价标准 ：不仅要有效可行的实验估计方法；还需要有度量模型泛化能力的评价标准。
- 反映了任务需求，对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果。
- 回归任务最常用的性能度量是**“均方误差”**（MSE:mean squared error）
  - 给定样例集$D = \{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\}$，要估计学习器$f$的性能，就要把机器学习预测结果$f(x)$与真实标记$y$进行比较。
  - 均方误差：$E(f;D) = \frac{1}{m}\sum_{i=1}^m {(f(x_i)-y_i)^2}$
  - 更一般的，对于数据分布$D$和概率密度函数$p(·)$，均方误差可以描述为：$E(f;D)=\int_D  {(f(x)-y)^2p(x)}\,{\rm d}x$

#### 2.3.1 错误率与精度

- 错误率是分类错误的样本数占样本总数的比例。
  - 对样例集$D$，分类错误率定义为$E(f;D)=\frac{1}{m}\sum_{i=1}^m{\prod(f(x_i)\neq y_i)}$
  - 更一般的对于数据分布$D$和概率密度函数$p(·)$，错误率可以描述为$E(f;D)=\int_D{\prod(f(x)\neq y)p(x)} \,{\rm d}x$
- 精度是分类正确的样本数占样本总数的比例。对样例集$D$，精度定义为$acc(f;D)=\frac1m\sum_{i=1}^m\prod(f(x_i)=y_i)=1-E(f;D)$
  - 对于分布$D$和概率密度函数$p(·)$，准确率可以描述为$acc(f;D)=\int_D\prod(f(x)=y)p(x)\,{\rm d}x=1-E(f;D)$

#### 2.3.2 差错率、查全率与$F1$

- 关心检索出的信息有多少比例是用户感兴趣的。用户感兴趣的信息有多少被检索出来了。
- 二分类问题，根据样例真实类别与学习器预测类别组合划分为分类结果混淆矩阵（confusion matrix）：

| 真实情况 |  预测为正   |  预测为反   |
| :--: | :-----: | :-----: |
|  正例  | TP（真正例） | FN（假反例） |
|  反例  | FP（假正例） | TN（真反例） |

- 查准率和查全率是一对矛盾的度量

  - 查准率高时，查全率往往偏低。

- | 指标                        | 公式                                                | 备注                                       |
  | --------------------------- | --------------------------------------------------- | ------------------------------------------ |
  | 准确率（Accuracy）          | $A=\frac{TP+TN}{TP+FN+FP+TN}$                       | 在所有的预测案例中，预测准确的比例         |
  | 错误率                      | $E=\frac{FP+FN}{TP+FP+TN+FN}=1-A$                   | 在所有的预测案例中，预测错误的比例         |
  | 敏感性（Sensitivity）       | $Sensitivity=\frac{TP}{Realtrue}=\frac{TP}{TP+FN}$  | 全部实际分类为正的样本中，被准确预测的比例 |
  | 特异性（Specitivity）       | $Specitivity=\frac{TN}{RealFalse}=\frac{TN}{TN+FP}$ | 全部实际分类为反的样本中，被准确预测的比例 |
  | 精确率、查准率（Precision） | $P=\frac{TP}{PredictiTrue}=\frac{TP}{TP+FP}$        | 所有预测为正的样本中，准确预测的比例。     |
  | 召回率、查全率（Recal）     | $R=\frac{TP}{TP+FN}=Sensitivity$                    | 有多少正确的结果被正确归类。等于敏感性     |

- P-R曲线

  ![image-20180825200851484](assets/image-20180825200851484.png)

  - 查准率为纵轴，查全率为横轴
  - 若一个学习器的P-R曲线被另一个学习器曲线完全包住，则可断言后者的性能优于前者。
  - 如果P-R曲线有交叉，一般情况下比较曲线下面积的大小。
    - 平衡点（Break-Event Point）：查全率=查准率时的取值。更常用的还是F1。
    - F1度量$F1=\frac{2×P×R}{P+R}=\frac{2×TP}{样例总数+TP-TN}$。
      - 基于查准率和查全率的调和平均$\frac{1}{F1}=\frac12·(\frac1P+\frac1R)$
      - 调和平均更重视较小值。
    - $F1$值的般形式$F_\beta$=$\frac{(1+𝛽^2)×P×R}{(𝛽^2×P)+R}$
      - $F_𝛽 $是加权调和平均：$\frac{1}{F_𝛽}=\frac{1}{1+𝛽^2}·(\frac1P+\frac{𝛽^2}{R})$
        - $𝛽>0$度量了查全率对查准率的相对重要性；
        - $𝛽=1$退化为标准的$F1$；
        - $𝛽>1$时查全率有更大影响；
        - $𝛽<1$时查准率有更大影响。

- 差准与查全的侧重：
  - 商品推荐的时候，尽可能打扰用户，只显示感兴趣的内容，侧重查准率
  - 逃犯信息检索，尽可能少漏掉逃犯，查全更重要。

- 在n个二分类混淆矩阵上综合考虑查准率和查全率
  - 先在各混淆矩阵上分别计算出查准率和查全率，记为$(P_1,R_1),(P_2,R_2),...(P_n,R_n)$，再计算平均值，得到“宏查准率”（macro-P），“宏查全率”（macro-R），以及“宏F1”（macro-F1）
    - $macro-P=\frac1n\sum_{i=1}^nP_i$
    - $macro-R=\frac1n\sum_{i=1}^nR_i$
    - $macro-F1=\frac{2×macro-P×macro-R}{macro-P+macro-R}$
  - 可以先将各混淆矩阵对应元素进行平均，得到TP、FP、TN、FN的平均值，分别记为$\bar{TP}、\bar{FP}、\bar{TN}、\bar{FN}$，再基于这些平均值计算出“微查准率”（micro-P）、“微查全率”（mirco-R）、“微F1”（micro-F1）：
    - $micro-P=\frac{\bar{TP}}{\bar{TP}+\bar{FP}}$
    - $micro-R=\frac{\bar{TP}}{\bar{TP}+\bar{FN}}$
    - $micro-F1=\frac{2×micro-P×micro-R}{micro-P+micro-R}$

#### 2.3.3 ROC与AUC

- ROC（Receiver Operating Characteristic）受试者工作特征曲线

  - “真正例率”（True Positive Rate，TPR，分母为所有实际正例样本）为纵轴；$TPR=\frac{TP}{TP+FN}$
  - “假正例率”（False Positive Rate，FPR，分母为所有实际反例样本）为横轴；$FPR=\frac{FP}{FP+TN}$

  ![1536128206643](F:/Temp/1536128206643.png)

  - ROC曲线是学习器在正例上预测正确以及在反例上预测错误的综合表现。
  - 点$(0,1)$对应于将所有正例排在所有反例之前的“理想模型”。（所有样本预测全部正确）
  - 若一个学习器ROC曲线被另一个学习器曲线完全包住，则可断言后者的性能优于前者。
  - 若两个学习器的ROC曲线发生交叉，较为合理的判据是比较ROC曲线下的面积，即AUC（Area Under ROC Curve）。
    - $AUC=\frac12\sum_{i=1}^{m-1}(x_{i+1}-x_i)·(y_i+y_{i+1})$
    - AUC考虑的是样本预测的排序质量，与排序误差联系紧密。

#### 2.3.4 代价敏感错误率和错误曲线

- 为权衡不同类型错误所造成的不同损失，可谓错误赋予“非均等代价”（unequal cost）
- 二分类任务可以设定代价矩阵（cost matrix）

| 真实类别 |   预测为第0类    |   预测为第1类    |
| :--: | :---------: | :---------: |
| 第0类  |      0      | $cost_{01}$ |
| 第1类  | $cost_{10}$ |      1      |

- $cost_{ij}$表示将第i类样本预测为第j类样本的代价。$cost_{ii}=0$
- 在非均等代价下，我们所希望的是**最小化“总体代价”**（total cost）
- 将第0类作为正类，将第1类作为反类，令$D^+$与$D^-$分别代表样例集$D$的正例子集和反例子集，“代价敏感”（cost-sensitive）错误率：$E(f;D;cost)=\frac1m(\sum_{x_i\in{D^+}}\prod(f(x_i)\neq y_i)×cost_{01}+\sum_{x_i\in{D^-}}\prod(f(x_i)\neq y_i)×cost_{10})$
- 非均等代价下，ROC曲线不能直接反映出学习器的期望总体代价，用代价曲线（cost curve）表示。
  - 代价曲线图横轴取值为$[0,1]$的**正例概率代价**：$P(+)cost=\frac{p×cost_{01}}{p×cost_{01}+(1-p)×cost{10}}$
    - $p$是样例为正例的概率。
  - 纵轴是取值为$[0,1]$的归一化代价：$cost_{norm}=\frac{FNR×p×cost_{01}+FPR×(1-p)×cost_{10}}{p×cost_{01}+(1-p)×cost_{10}}$
    - FPR是假正例率；FNR是假反例率。
- ROC曲线上每一点对应了代价平面上的一条线段。设ROC曲线上点的坐标为（TPR，FPR），计算得到FNR，在代价平面上绘制一条从（0，FPR）到（1，FNR）的线段，线段下的面积即表示了该条件下的期望总体代价。
  - 将ROC曲线上每个点转化为代价平面上的一个线段，然后去所有线段的下界，围城的面积即为在所有条件下学习器的期望总体代价。

![1536131833331](F:/Temp/1536131833331.png)

### 2.4 比较检验

- 对学习器的性能进行评估比较：
  - 使用某种实验评估方法测的学习器的某个性能度量结果。
  - 对这些结果进行比较。
- 机器学习性能比较：
  - 希望比较的是泛化性能，实验评估方法获得的是测试集上的性能。
  - 测试集的性能与测试集的选择关系很大。
  - 机器学习算法本身有一定的随机性

#### 2.4.1 假设检验（对单个学习器泛化性能进行检验）

- 假设是对学习器泛化错误率分布的某种判断或猜想。
- 现实任务中，我们无法知道学习器的泛化错误率，只能获知其测试错误率$\hat \epsilon$，泛化错误率与测试错误率未必相同。
- 可以根据测试错误率估推出泛化错误率的分布。
  - 泛化错误率$\epsilon$意味着学习器在一个样本上犯错的概率为$\epsilon$；

  - 测试错误率$\hat\epsilon$意味着在$m$个测试样本中恰有$m×\hat\epsilon$个被误分类；

  - 假定测试样本是从样本总体分布中独立采样得到，那么泛化错误率为$\epsilon$的学习器将其中$m’$个样本误分类，其余样本全都分类正确的概率是$\epsilon^{m'}(1-\epsilon)^{m-m'}$

  - 学习器恰将$\hat\epsilon×m$个样本误分类的概率为：（这同时也是在包含$m$个样本的测试集上，泛化错误率为$\epsilon$的学习器被测得测试错误率为$\hat\epsilon$的概率）$P(\hat\epsilon;\epsilon)=\bigl(\begin{smallmatrix}m\\\hat\epsilon×m\end{smallmatrix}\bigr)\epsilon^{\hat\epsilon×m}(1-\epsilon)^{m-\hat\epsilon×m}$。

    - 在$\epsilon=\hat\epsilon$时，$P(\epsilon;\hat\epsilon)$最大；在$|\epsilon-\hat\epsilon|$增大时，$P(\epsilon;\hat\epsilon)$减小，符合二项分布（binomial）

    - 可以使用二项检验（binomial test）对$\epsilon≤\epsilon_0$进行检验。在$1-\alpha$的概率内所能观测到的最大错误概率为：$\bar\epsilon=max\text{ }s.t.\text{ }\sum_{i=\epsilon_0\times m+1}^mC_m^i\epsilon^i(1-\epsilon)^{m-i}<\alpha$，$1-\alpha$反映了结论的置信度（confidence）

      - 若测试错误率$\hat\epsilon$小于临界值$\bar\epsilon$，根据二项检验可以得出结论：在$\alpha$的显著度下，假设$\epsilon\leq\epsilon_0$不能被拒绝，即能以$1-\alpha$的置信度认为，学习器的泛化错误率不大于$\epsilon_0$；否则假设可以被拒绝，即在$\alpha$的显著度下可认为学习器的泛化错误率大于$\epsilon_0$。

    - t检验（t-test）：通过多次重复留出法或交叉验证法进行多次训练/测试，得到多个测试错误率。
      - $k$次测试得到$k$个测试错误率，$\hat\epsilon_1,\hat\epsilon_2,\hat\epsilon_3,…,\hat\epsilon_k$； 
      - 平均测试错误率$𝜇 =\frac1k\sum_{i=1}^k\hat\epsilon_i$
      - 测试错误率方差$\sigma^2=\frac{1}{k-1}\sum_{i=1}^k(\hat\epsilon_i-𝜇)^2$
      - $k$个测试错误率可以看做泛化错误率$\epsilon_0$的独立采样，$\tau_t=\frac{\sqrt{k}(𝜇-\epsilon_0)}{\sigma}$服从自由度为$k-1$的$t$分布。

      ![image-20180906213126114](assets/image-20180906213126114.png)

#### 2.4.2 交叉验证t检验

- 交叉验证t检验，比较两个学习器的性能。
- 对两个学习器A和B，使用$k$折交叉验证法得到测试错误率分别为$\epsilon^A_1,\epsilon^a_2,\epsilon^A_3,…,\epsilon^A_k$和$\epsilon^B_1,\epsilon^B_2,\epsilon^B_3,…\epsilon^B_k$，其中$\epsilon^A_i$和$\epsilon^B_i$实在相同的第i折训练/测试集上得到的结果。
- 采用$k$折交叉验证“成对$t$检验”（paired t-tests）进行比较检验。（若两个学习器性能相同，则它们使用相同的训练/测试集得到的测试错误率应相同。）
  - 先对每对结果求差，$\Delta_i=\epsilon^A_i-\epsilon^B_i$，若两个学习器性能相同，差值均值应该为0；
  - 根据差值$\Delta_1,\Delta_2,\Delta_3,…,\Delta_k$来对“学习器A和B性能相同”这个假设做$t$检验，计算差值的均值$\mu$和$\sigma^2$
  - 在置信区间$\alpha$下，若变量$\tau_t=|\frac{\sqrt{k}\mu}{\sigma}|$＜临界值$t_{\alpha/2,k-1}$，则假设不能被拒绝，即认为两个学习器的性能没有明显区别。
  - 使用交叉验证实验估计方法时，不同轮次的训练集会有一定程度的重叠，是的测试错误率实际上不独立，会导致过高估计假设成立的概率。
    - 采用$5×2$交叉验证法：做5次2折交叉验证。每次2折交叉验证之前随机将数据打乱，使得5次交叉验证中的数据划分不重复。
    - 对两个学习器A和B，第i次2折交叉验证将产生两对测试错误率，对它们分别求差，得到第1折上的差值$\Delta^1_i$和第2折上的差值$\Delta^2_i$
    - 为缓解测试错误率的非独立性，我们仅计算第1折2次交叉验证的两个结果的平均值$\mu=0.5(\Delta^1_1+\Delta^2_1)$
    - 对每次2折实验结果都计算方差$\sigma^2_i=(\Delta^1_i-\frac{\Delta^1_i+\Delta^2_i}{2})^2+(\Delta^2_i-\frac{\Delta^1_i+\Delta^2_i}{2})^2$
    - 变量$\tau_t=\frac{\mu}{\sqrt{0.2\sum_{i=1}^5\sigma^2_i}}$服从自由度为5的t分布，其双边检验的临界值$t_{\alpha/2,5}$当$\alpha=0.05$时为2.5706，当$\alpha=0.1$时为2.0150。

#### 2.4.3 McNemar检验

- 二分类问题，使用留出法可以获得两学习器A和B分类结果的差别。即两者都正确、都错误、一个正确另一个错误的样本数——分类差别列联表（contingency table）

| 算法B  | 算法A正确    | 算法A错误    |
| ---- | -------- | -------- |
| 正确   | $e_{00}$ | $e_{01}$ |
| 错误   | $e_{10}$ | $e_{11}$ |

- 若假设两个学习器性能相同，则应当有$e_{01}=e_{10}$，那么变量$|e_{01}-e_{10}|$应当服从正态分布，且均值为1，方差为$e_{01}+e_{10}$。
- 变量$\tau_{\chi^2}=\frac{(|e_{01}-e_{10}|-1)^2}{e_{01}+e_{10}}$服从自由度为1的$\chi^2$分布，即标准正态分布变量的平方。
- 自由度为1的$\chi^2$检验的临界值当$\alpha=0.05$时为3.8415，当$\alpha=0.1$时为2.7055。

#### 2.4.4 Friedman检验和Nemenyi后续检验

- 在一组数据集上对多个算法进行比较：
  - 一种做法，在每个数据及上分别列出两两比较的结果；
  - 使用基于算法排序的Freidman检验。
- 假定用$D_1,D_2,D_3和D_4$四个数据集对算法ABC进行比较：
  - 首先用留出法或交叉验证法得到每个算法在每个数据及上的测试结果；
  - 在每个数据及上根据测试性能由好到坏排序，并赋予序值1,2,…；
  - 若算法的测试性能相同，则平分序值。得到算法比较序值表，以及平均序值。
  - 使用Friedman检验来判断这些算法是否性能都相同。
  - 若“所有算法的性能相同”这个假设被拒绝，说明算法的性能显著不同，需要进行后续检验（post-hoc test）

### 2.5 偏差与方差

- 偏差方差分解（bias-variance decomposition）是解释学习算法泛化性能的一种重要工具。
  - 对学习算法的期望泛化错误率进行拆解。
  - 对测试样本$x$，令$y_D$为$x$在数据集中的标记，$y$为$x$的真实标记，$f(x;D)$为训练集$D$上学习模型$f$在$x$上的预测输出。
    - 回归任务中，学习算法的期望预测为：$\bar{f(x)}=E_D[f(x;D)]$
    - 使用样本数相同的不同训练集产生的方差：$var(x)=E_D[(f(x;D)-\bar{f(x))^2}]$
    - 噪声为：$\epsilon^2=E_D[(y_D-y)^2]$
    - 期望输出与真是标记的差别为偏差（bias）：$bias^2=(\bar{f(x)-y})^2$​
  - 泛化误差可分解为偏差、方差与噪声之和。
    - 偏差度量学习算法的期望预测与真实结果的偏离程度，刻画了学习算法本身的你和能力；
    - 方差度量了同样大小的训练集的变动所导致的学习性能的变化，刻画了数据扰动所造成的影响；
    - 噪声表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，刻画了学习问题本身的难度。

# 习题

- 2.1 数据集包含1000个样本，其中500个正例，500个反例，将其划分为包含70%样本的训练集和30%样本的测试集用于留出法评估，试估算共有多少种划分方式。
  - 解答：$500×70%=350,500-350-150$，可取的划分方法共${C_{500}^{150}}^2$
- 2.2 数据集包含 100 个样本，其中正、反例各一半，假定学习算法所产生的模型是将新样本预测为训练样本数较多的类别(训练样本数相同时进行随机猜测)，试给出用10折交叉验证法和留一法分别对错误率进行评估所得的结果。
  - 解答：10折交叉验证，由于每次训练样本中正反例的数目一样，所以将结果判断为正反例的概率也一样。错误率的期望为50%。留一法，如果验证样本是正例，则训练样本中反例数量多，留出的样本被预测为反例；如果验证样本是反例，则训练样本中正例数量多，留出的样本被预测为正例，错误率为100%。
- 2.3 若学习器A的F1值，比学习器B高，试析A的BEP值是否也比B高？
  - 解答：$$F_{1A}>F_{1B}\lrarr\frac{2\times P_A\times R_A}{P_A+R_A}>\frac{2\times P_B\times \R_B}{P_B+R_B}$$。$BEP$是查准率与查全率相等时的取值，即$BEP=P=R$，令$P_A=R_A=BEP_A$，$P_B=R_B=BEP_B$，因此得到$\frac{2\times BEP_A^2}{2\times BEP_A}>\frac{2\times BEP_B^2}{2\times BEP_B}\rarr BEP_A>BEP_B$。即$F_1$值高的学习器，$BEP$值也高。
- 2.4 试述真正例率（TPR），假正例率（FPR）与查准率（P）、查全率（R）之间的关系。
  - 解答：真正例率是所有正例样本中，被正确预测的比例，$TPR=\frac{TP}{TP+FN}$；假正例率是所有反例样本中，预测错误的比例，$FPR=\frac{FP}{TN+FP}$；查准率是所有预测为正的样本中，准确预测的比例$P=\frac{TP}{TP+FP}$；查全率=真正例率。
- 2.5 证明$AUC=1-l_{rank}​$
  - 解答：![image-20180906215821992](assets/image-20180906215821992.png)
- 2.6 试述错误率与ROC曲线的联系
  - 解答：错误率$\epsilon=\frac{FP+FN}{m}$，其中$m$为样本的个数，$FP$为假正例的个数，$FN$为假反例的个数。给定的测试集，正反例数目固定，以ROC曲线上一个点作为阈值划分预测的正例和反例，该点的真正例率越高，假反例率就会越低；该点的假正例率越低，那么假正例越少，错误率也会降低。所以，ROC曲线上距离左上角（0,1）点越近，错误率越低。
- 2.7 试证明任意一条ROC曲线都有一条代价曲线与之对应，反之亦然。
  - 解答：求ROC曲线上每一点对应的FPR和FNR，在代价平面连接点 (0,FPR) 和点 (1，FNR)，代价曲线由所有连线的下界确定。由于ROC曲线是连续的，故必然有一条对应的代价曲线。 反过来，对代价曲线上的任何一点作切线，都可以得到对应的FPR与FNR，从而计算出ROC曲线上对应点的坐标。因此代价曲线也必然有一条对应的ROC曲线。
- 2.8 分析Min-Max规范化和z-score规范化的优缺点
  - 解答：Min-Max规范化比较简单，而且保证规范化后**所有元素都是正**的，每当有新的元素进来，只有在该元素大于最大值或者小于最小值时才要重新计算全部元素。但是若存在一个极大(小)的元素，会导致其他元素变的非常小(大)；z-score规范化能把数据转化为标准正态分布，对个别极端元素不敏感，这种规范化的缺点就是改变了数据的分布，这可能会引入某种误差。
- 2.9 描述$\chi^2$检验过程
  - 解答：$\chi^2$检验适用于方差的检验，是一种统计量的分布在零假设成立时近似服从卡方分布（$ \chi ^{2} $分布）的假设检验。步骤如下：
    - 分均值已知和均值未知两种情况，求得卡方检验统计量；
    - 根据备选假设以及$\alpha$，求得所选假设对应的拒绝域（临界值区间）；
    - 根据第一步中求得的卡方统计量与第二步中求得的拒绝域，判断假设成立与否；
- 2.10 试述Friedman检验中使用两种$\tau$的区别。



